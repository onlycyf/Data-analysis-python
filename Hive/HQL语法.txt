HQL语句

创建数据库
create database if not exists 数据库名称 [location /位置/名称]

查看所有数据库或者表名称
show databases;
show tables;

查看数据库或者表详细信息(extended查看扩展信息，通过alter加入的字段通过此方式查看)
desc database [extended] 数据库名称
desc 表名称

切换数据库
use 数据库名称

修改数据库(数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置)
alter database 数据库名称 set dbproperties('createtime'='20170503')

添加分区
alter table 表名称 add partition(分区字段=value);

添加一列字段并加上注释
alter table 表名 add columns(字段名 类型 comment '注释');

重命名表
alter table 原表名 rename to 新表名

更新列
alter table 表名称 change [cloumn] 旧的列名称 新的列名称 列类型 [COMMENT 列注释] [FIRST|AFTER column_name]

创建表
create [external] table if not exists 表名(
列名 数据类型 [comment 本列注释], 
......
)
[comment 表注释]
[partitioned by (列名 数据类型 [comment 本列注释, ...])] # 分区
[clustered by (列名, 列名, ...)] # 分桶
[sorted by (列名 [asc|desc], ...) info num_buckets buckets] # 排序
[row format row_format] # map 列名等字段的分隔符
[stored as file_format] # 按某格式存储
[location hdfs_path] # 表的存储位置
[tblproperties (property_name=property_value, ...)]
[as select_statement]

修复分区表
msck repair table 表名称;

查询表
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
  FROM table_reference
  [WHERE where_condition]
  [GROUP BY col_list]
  [ORDER BY col_list]
  [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]]
 [LIMIT number]

清空表数据
truncate table 表名

加载数据
load data [local] inpath '文件地址' [overwrite] into table 表名 [patition(分区字段=value, ...)];

上传数据
dfs -put '文件路径' 'hdfs上想要放文件的路径'

插入数据
insert into table 表名称 [partition(分区字段=value)] values(value, value)

根据单表查询结果插入
insert overwrite|into table 表名称 [partition(分区字段=value)] select 字段名, 字段名 from 表名 where 条件;

多插入模式(把一张表的数据插入到其他多个表中)
from 表名
insert overwrite|into table 表1名称 [partition(分区字段=value)]
select 字段名, 字段名 where 条件
insert overwrite|into table 表2名称 [partition(分区字段=value)]
select 字段名, 字段名 where 条件

导出数据到本地/HDFS上
insert overwrite [local] directory '本地路径'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select 查询字段 from 表名;

行转列
孙悟空 白羊座 A
大海  射手座 A
宋宋  白羊座 B
猪八戒 白羊座 A
凤姐  射手座 A

将上面的数据变成下面的样子

射手座,A            大海|凤姐
白羊座,A            孙悟空|猪八戒
白羊座,B            宋宋

concat(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;
concat_ws(separator, str1, str2,...)：它是一个特殊形式的 CONCAT()。
第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，
返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。
分隔符将被加到被连接的字符串之间;
collect_set(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，
产生array类型字段。

获取hdsf中的json类型的字典中的键
例如: {'a': 11, 'b': 22}
select get_json_object(列名称, '$.字典中键名称') from 表名称;
列转行

explode(col)：将hive一列中复杂的array或者map结构拆分成多行。

movie   category
《疑犯追踪》  悬疑,动作,科幻,剧情
《Lie to me》 悬疑,警匪,动作,心理,剧情
《战狼2》   战争,动作,灾难

将上面的数据变成如下结果
《疑犯追踪》      剧情
《Lie to me》   悬疑
《Lie to me》   警匪
《Lie to me》   动作
《Lie to me》   心理
《Lie to me》   剧情
《战狼2》        战争
《战狼2》        动作
《战狼2》        灾难
select movie, category_name from movie_info
lateral view explode(category) table_tmp as category_name;

如果是json类型的数组里包含着字典,使用行转列之前，
需要使用正则匹配将数组两边的[]去掉，然后将字典之间的分隔符替换成hive中的特殊分隔符\001，最后在使用lateral view explode()
例如：取出下面所有的amount
      id      ext
     1001    {"assume_discount":[{"amount":0,"assume":1,"type":"ticket_coupon"},{"amount":"1.51","assume":1,"type":"goods_discount"}]}
SELECT get_json_object(重命名列名称, '$.amount') 重命名
FROM 表名称
lateral view explode(split(regexp_replace(regexp_replace(get_json_object(字段名称(ext),'$.键名称(assume_discount)'),
                                                                         '^\\[|\\]$',''), # 将数组开头的[和结尾的]替换成空
                                                        '\\}\\,\\{', '\\}\\\\001\\{'), #将字典之间的分隔符替换成\001(hive特殊分隔符，注意前面是4个\)
                      '\\\\001')) 新列名称 as 重命名列名称

将单词转小写函数：lower 或者 lcase 函数
select lower('App') from 表名  #结果是：app

将单词转大写：upper 或者 ucase 函数
select upper('App') from 表名 # 结果是app

求字符串长度 length 函数
select length('App')  #求字符串长度：3

左补齐 lpad 函数
select lpad('what', 10, '*')  #左补齐，长度补成10位，以'*'补充，结果是：******what

右补齐 rpad 函数
select rpad('what', 10, '*')   #同理是右补齐

强制转换类型cast函数
select cast(29 as float)  #强制类型转换，转成float类型

拆分字符串 split 函数
select split('www#baidu#com', '#')  #以自定字符，例如'#'拆分字符串

时间戳格式化标准时间函数
select from_unixtime(1441565203,'yyyy/MM/dd HH:mm:ss') from 表名;
1441565203会转成2015-09-07 02:46:43

标准时间格式化时间戳
select unix_timestamp('2015-09-07 02:46:43') from 表名;
2015-09-07 02:46:43会转化成1441565203


窗口函数
over()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化
current row：当前行
n preceding：往前n行数据
n following：往后n行数据
unbounded：起点, unbounded preceding 表示从前面的起点，
                 unbounded following表示到后面的终点
lag(col,n)：往前第n行数据
lead(col,n)：往后第n行数据
ntile(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，
编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。

排序
order by 全局排序,一个reducer
sort by 每个MapReduce内部排序
distribute by 分区，结合sort by使用

分桶时需要设置下面参数
set hive.enforce.bucketing=true;
set mapreduce.job.reduces=-1;

查询分通表
select 字段 from 表名 tablesample(bucket x out of y on 字段);
y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。
例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，
当y=8时，抽取(4/8=)1/2个bucket的数据。
x表示从哪个bucket开始抽取，如果需要取多个分区，
以后的分区号为当前分区号加上y。例如，table总bucket数为4，
tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，
抽取第1(x)个和第3(x+y)个bucket的数据。

给为null的数据赋值
nvl(字段1, replace_with)
如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，
如果两个参数都为NULL ，则返回NULL。

case 表字段 when value1 then value2 else value3 end
例如：性别为男设置为1，否则为0
case sex when '男' then 1 else 0 end

Rank
Rank()排序向同时会重复，总数不会变
dense_rank() 排序相同时会重复，总数会减少
row_number() 会根据顺序计算
select *, 
rank() over(partition by subject order by score desc),
dense_rank() over(partition by subject order by score desc),
row_number() over(partition by subject order by score desc)
from sroce limit 100; 

hadoop压缩配置
先关闭集群和hive
将编译好的hadoop源码编译支持Snappy的包配置到hadoop-2.7.2/lib/native上
重新启动hadoop集群和hive
开启Map输出阶段压缩
1．开启hive中间传输数据压缩功能
hive (default)>set hive.exec.compress.intermediate=true;
2．开启mapreduce中map输出压缩功能
hive (default)>set mapreduce.map.output.compress=true;
3．设置mapreduce中map输出数据的压缩方式
hive (default)>set mapreduce.map.output.compress.codec=
 org.apache.hadoop.io.compress.SnappyCodec;
开启Reduce输出阶段压缩
1．开启hive最终输出数据压缩功能
hive (default)>set hive.exec.compress.output=true;
2．开启mapreduce最终输出数据压缩
hive (default)>set mapreduce.output.fileoutputformat.compress=true;
3．设置mapreduce最终数据输出压缩方式
hive (default)> set mapreduce.output.fileoutputformat.compress.codec =
 org.apache.hadoop.io.compress.SnappyCodec;
4．设置mapreduce最终数据输出压缩为块压缩
hive (default)> set mapreduce.output.fileoutputformat.compress.type=BLOCK;

存储格式
常用的4种格式
1.TextFile格式：textfile 、sequencefile、orc、parquet
默认格式，数据不做压缩，磁盘开销大，数据解析开销大。
可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，
从而无法对数据进行并行操作
2.Orc格式：
每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，
这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，
这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，
分别是Index Data，Row Data，Stripe Footer
3. Parquet格式：
Parquet文件是以二进制方式存储的，所以是不可以直接读取的，
文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。
通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，
由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，
这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。

--------------------企业调优----------------------
Fetch抓取：
Hive中对某些情况的查询可以不必使用MapReduce计算
set hive.fetch.task.conversion=more
老版本hive默认是minimal，该属性修改为more以后，
在全局查找、字段查找、limit查找等都不走mapreduce。

本地模式：
大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。
不过，有时Hive的输入数据量是非常小的。在这种情况下，
为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。
对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。
对于小数据集，执行时间可以明显被缩短。
set hive.exec.mode.local.auto=true;

表的优化
将key相对分散，并且数据量小的表放在join的左边，
这样可以有效减少内存溢出错误发生的几率；再进一步，
可以使用map join让小的维度表（1000条以下的记录条数）先进内存。
在map端完成reduce。
实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。
小表放在左边和右边已经没有明显区别。
